from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import InvalidSessionIdException, WebDriverException, StaleElementReferenceException

import pandas as pd
import time
import datetime
import warnings
import requests
import json
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

warnings.filterwarnings('ignore')

# ============================================
# ì„¤ì •
# ============================================
URL = "https://www.wanted.co.kr/wdlist/518/10110"   # ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´
#URL = "https://www.wanted.co.kr/wdlist/518/899"      # íŒŒì´ì¬ ê°œë°œì 
DEFAULT_JOB_CATEGORY = 'ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´'

MAX_WORKERS = 10  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜

# ============================================
# 1ë‹¨ê³„: Seleniumìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ë° ê³µê³  ëª©ë¡ ìˆ˜ì§‘
# ============================================
print("=" * 70)
print("ì›í‹°ë“œ ì±„ìš©ê³µê³  í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬ ë²„ì „) ğŸš€")
print("=" * 70)

chrome_options = webdriver.ChromeOptions()
chrome_options.add_experimental_option("detach", True)
chrome_options.add_argument('--start-maximized')

driver = webdriver.Chrome(options=chrome_options)
driver.get(URL)
time.sleep(3)

# XPath ì„ íƒì
element_xpath = "//div[@data-cy='job-card']/a"
alternative_xpaths = [
    "//div[@data-cy='job-card']/a",
    "//a[contains(@href, '/wd/')]",
    "//div[contains(@class, 'JobCard_JobCard')]/a",
]

wait = WebDriverWait(driver, 20)

# ìš”ì†Œ ì°¾ê¸°
element_found = False
for xpath in alternative_xpaths:
    try:
        print(f"ìš”ì†Œ ì°¾ê¸° ì‹œë„ ì¤‘: {xpath}")
        wait.until(EC.presence_of_element_located((By.XPATH, xpath)))
        element_xpath = xpath
        element_found = True
        print(f"âœ“ ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤: {xpath}")
        break
    except Exception as e:
        print(f"âœ— íƒ€ì„ì•„ì›ƒ: {xpath}")
        continue

if not element_found:
    print("ê²½ê³ : ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ XPathë¡œ ì§„í–‰...")
    element_xpath = alternative_xpaths[0]

# ìŠ¤í¬ë¡¤ ë‹¤ìš´
SCROLL_PAUSE_TIME = 1.5
try:
    last_height = driver.execute_script("return document.body.scrollHeight")
except InvalidSessionIdException:
    print("ì„¸ì…˜ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.")
    driver.quit()
    raise

same_count = 0
print("\n[1ë‹¨ê³„] ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì¤‘...")

while True:
    try:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(SCROLL_PAUSE_TIME)
        new_height = driver.execute_script("return document.body.scrollHeight")
    except (InvalidSessionIdException, WebDriverException) as e:
        print("ë¸Œë¼ìš°ì € ì„¸ì…˜ì´ ëŠê²¼ìŠµë‹ˆë‹¤.")
        break

    if new_height == last_height:
        same_count += 1
    else:
        same_count = 0

    if same_count >= 2:
        print("âœ“ ìŠ¤í¬ë¡¤ ì™„ë£Œ - ë” ì´ìƒ ìƒˆ ê³µê³  ì—†ìŒ")
        break

    last_height = new_height

# ìš”ì†Œ ìˆ˜ì§‘
elements = []
try:
    elements = driver.find_elements(By.XPATH, element_xpath)
    print(f"âœ“ {len(elements)}ê°œ ê³µê³  ë°œê²¬")
except Exception as e:
    print(f"ìš”ì†Œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    elements = []

# ë¦¬ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘
list_data = []
for idx, e in enumerate(elements):
    try:
        max_retries = 3
        retry_count = 0
        href = None
        job_category_id = ''
        company_id = ''
        company_name = ''
        position_name = ''
        position_id = ''
        
        while retry_count < max_retries:
            try:
                href = e.get_attribute('href')
                if href and href.startswith('/'):
                    href = f"https://www.wanted.co.kr{href}"
                
                parent_div = e.find_element(By.XPATH, "./..")
                try:
                    button = parent_div.find_element(By.XPATH, ".//button[@data-attribute-id='position__bookmark__click']")
                    job_category_id = button.get_attribute('data-job-category-id') or ''
                    company_id = button.get_attribute('data-company-id') or ''
                    company_name = button.get_attribute('data-company-name') or ''
                    position_name = button.get_attribute('data-position-name') or ''
                    position_id = button.get_attribute('data-position-id') or ''
                except:
                    pass
                break
            except StaleElementReferenceException:
                retry_count += 1
                if retry_count < max_retries:
                    elements = driver.find_elements(By.XPATH, element_xpath)
                    if idx < len(elements):
                        e = elements[idx]
                    time.sleep(0.5)
                else:
                    raise
        
        if href is None:
            continue
        
        list_data.append({
            'job_category_id': job_category_id,
            'job_category': DEFAULT_JOB_CATEGORY,
            'company_id': company_id,
            'company_name': company_name,
            'position_name': position_name,
            'position_id': position_id,
            'link': href or '',
        })
        
        if (idx + 1) % 50 == 0:
            print(f"  [{idx+1}/{len(elements)}] ë¦¬ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    
    except Exception as ex:
        continue

print(f"âœ“ ë¦¬ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(list_data)}ê°œ")

# 1ë‹¨ê³„ ì—‘ì…€ ì €ì¥
df_list = pd.DataFrame(list_data)
now_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
list_save_path = f"C://Users//MULTICAMPUS//Desktop//curosr-playground//wanted//1.{DEFAULT_JOB_CATEGORY}_wanted_{now_str}.xlsx"
df_list.to_excel(list_save_path, index=False, engine='openpyxl')
print(f"âœ“ ë¦¬ìŠ¤íŠ¸ ì—‘ì…€ ì €ì¥: {list_save_path}")

# Selenium ë¸Œë¼ìš°ì € ì¢…ë£Œ (2ë‹¨ê³„ëŠ” requests ì‚¬ìš©)
print("\në¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘... (2ë‹¨ê³„ëŠ” requests ì‚¬ìš©)")
driver.quit()

# ============================================
# 2ë‹¨ê³„: requests + ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
# ============================================
print(f"\n[2ë‹¨ê³„] ìƒì„¸ í˜ì´ì§€ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ ({MAX_WORKERS}ê°œ ë™ì‹œ ì²˜ë¦¬) ğŸš€")

# ì§„í–‰ ìƒí™© ì¶”ì 
progress_lock = threading.Lock()
completed_count = 0

def crawl_detail_page(row_data):
    """ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (requests + __NEXT_DATA__ JSON)"""
    global completed_count
    
    idx = row_data['idx']
    href = row_data['link']
    company_name = row_data.get('company_name', '')
    
    result = {
        'idx': idx,
        'position': '',
        'content1': '',
        'content2': '',
        'content3': '-',
        'content4': '-',
        'period': '-',
        'skill': ''
    }
    
    if pd.isna(href) or not href:
        return result
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9',
        }
        
        response = requests.get(href, headers=headers, timeout=10)
        
        if response.status_code != 200:
            return result
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # __NEXT_DATA__ì—ì„œ JSON ì¶”ì¶œ
        next_data_script = soup.find('script', id='__NEXT_DATA__')
        
        if next_data_script:
            try:
                json_data = json.loads(next_data_script.string)
                props = json_data.get('props', {})
                page_props = props.get('pageProps', {})
                job_detail = page_props.get('jobDetail', {})
                
                # í¬ì§€ì…˜ ìƒì„¸
                result['position'] = job_detail.get('position', '')
                
                # ì£¼ìš”ì—…ë¬´, ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­, í˜œíƒ
                detail = job_detail.get('detail', {})
                
                # ì£¼ìš”ì—…ë¬´
                intro = detail.get('intro', '')
                result['content1'] = intro.replace('\n', ' ').replace('â€¢ ', '').strip() if intro else ''
                
                # ìê²©ìš”ê±´
                requirements = detail.get('requirements', '')
                result['content2'] = requirements.replace('\n', ' ').replace('â€¢ ', '').strip() if requirements else ''
                
                # ìš°ëŒ€ì‚¬í•­
                preferred = detail.get('preferred', '')
                result['content3'] = preferred.replace('\n', ' ').replace('â€¢ ', '').strip() if preferred else '-'
                
                # í˜œíƒ ë° ë³µì§€
                benefits = detail.get('benefits', '')
                result['content4'] = benefits.replace('\n', ' ').replace('â€¢ ', '').strip() if benefits else '-'
                
                # ë§ˆê°ì¼
                due_time = job_detail.get('dueTime', '')
                result['period'] = due_time if due_time else '-'
                
                # ê¸°ìˆ ìŠ¤íƒ
                skill_tags = job_detail.get('skillTags', [])
                if skill_tags:
                    result['skill'] = '::'.join([tag.get('name', '') for tag in skill_tags if tag.get('name')])
                
            except json.JSONDecodeError:
                pass
        
        # JSONì´ ì—†ìœ¼ë©´ HTML íŒŒì‹± í´ë°±
        if not result['content1']:
            try:
                # ì£¼ìš”ì—…ë¬´
                content1_div = soup.find('h3', string=lambda x: x and 'ì£¼ìš”ì—…ë¬´' in x)
                if content1_div:
                    parent = content1_div.find_parent('div')
                    if parent:
                        result['content1'] = parent.get_text(' ', strip=True).replace('ì£¼ìš”ì—…ë¬´', '').replace('â€¢ ', '').strip()
            except:
                pass
            
            try:
                # ìê²©ìš”ê±´
                content2_div = soup.find('h3', string=lambda x: x and 'ìê²©ìš”ê±´' in x)
                if content2_div:
                    parent = content2_div.find_parent('div')
                    if parent:
                        result['content2'] = parent.get_text(' ', strip=True).replace('ìê²©ìš”ê±´', '').replace('â€¢ ', '').strip()
            except:
                pass
    
    except Exception as e:
        pass
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    with progress_lock:
        completed_count += 1
        total = row_data['total']
        status = "âœ“" if result['content1'] or result['content2'] else "-"
        print(f"\r  [{completed_count}/{total}] ìƒì„¸ í¬ë¡¤ë§ ì¤‘... ({completed_count/total*100:.0f}%) {status}", end='', flush=True)
    
    return result

# ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
df_detail = pd.read_excel(list_save_path, engine='openpyxl')
total = len(df_detail)

# row_data ì¤€ë¹„
row_data_list = []
for idx, row in df_detail.iterrows():
    row_data_list.append({
        'idx': idx,
        'link': row['link'],
        'company_name': row.get('company_name', ''),
        'total': total
    })

detail_results = []
start_time = time.time()

with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    futures = [executor.submit(crawl_detail_page, rd) for rd in row_data_list]
    
    for future in as_completed(futures):
        detail_results.append(future.result())

# ê²°ê³¼ ì •ë ¬ (ì›ë˜ ìˆœì„œëŒ€ë¡œ)
detail_results.sort(key=lambda x: x['idx'])

elapsed_time = time.time() - start_time
print(f"\nâœ“ ìƒì„¸ í¬ë¡¤ë§ ì™„ë£Œ! ({elapsed_time:.1f}ì´ˆ, {elapsed_time/total:.2f}ì´ˆ/ê±´)")

# ============================================
# 3ë‹¨ê³„: ê²°ê³¼ í•©ì¹˜ê¸° ë° ì €ì¥
# ============================================
print(f"\n[3ë‹¨ê³„] ê²°ê³¼ ì €ì¥ ì¤‘...")

# ìƒì„¸ ì •ë³´ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
detail_info_list = [{k: v for k, v in r.items() if k != 'idx'} for r in detail_results]
df_detail_info = pd.DataFrame(detail_info_list)

# ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
df_final = pd.concat([df_detail, df_detail_info], axis=1)
df_final['job_category'] = DEFAULT_JOB_CATEGORY

# ì—‘ì…€ ì €ì¥ ì „ ë¶ˆë²• ë¬¸ì ì œê±° (openpyxl ì˜¤ë¥˜ ë°©ì§€)
import re
ILLEGAL_CHARS_RE = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f]')

def clean_illegal_chars(value):
    """ì—‘ì…€ì—ì„œ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ì œì–´ ë¬¸ì ì œê±°"""
    if isinstance(value, str):
        return ILLEGAL_CHARS_RE.sub('', value)
    return value

print("  â†’ ë¶ˆë²• ë¬¸ì ì •ë¦¬ ì¤‘...")
for col in df_final.columns:
    if df_final[col].dtype == 'object':
        df_final[col] = df_final[col].apply(clean_illegal_chars)

# ìµœì¢… ì €ì¥
final_save_path = f"C://Users//MULTICAMPUS//Desktop//curosr-playground//wanted//2.{DEFAULT_JOB_CATEGORY}_wanted_{now_str}.xlsx"
df_final.to_excel(final_save_path, index=False, engine='openpyxl')

# í†µê³„
content_count = sum(1 for r in detail_results if r['content1'] or r['content2'])

print("=" * 70)
print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼")
print("=" * 70)
print(f"ì´ ê³µê³  ìˆ˜: {len(df_final)}ê°œ")
print(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘: {content_count}ê°œ ({content_count/len(df_final)*100:.1f}%)")
print(f"ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ ({elapsed_time/total:.2f}ì´ˆ/ê±´)")
print(f"job_category: {DEFAULT_JOB_CATEGORY}")
print("=" * 70)
print(f"âœ“ ë¦¬ìŠ¤íŠ¸ íŒŒì¼: {list_save_path}")
print(f"âœ“ ìµœì¢… íŒŒì¼: {final_save_path}")
print("\nğŸ‰ ì‘ì—… ì™„ë£Œ!")
