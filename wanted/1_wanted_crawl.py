from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import InvalidSessionIdException, WebDriverException, StaleElementReferenceException

import pandas as pd
import time
import datetime
import warnings
import requests
import json
import re
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

warnings.filterwarnings('ignore')

# ============================================
# ì„¤ì •
# ============================================
# í¬ë¡¤ë§í•  ì§ë¬´ ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ì§ë¬´ëª…, URL)
JOB_CATEGORIES = [
    # ê°œë°œë°œ ì¹´í…Œê³ ë¦¬

    # ('ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/10110'),
    # ('ì„œë²„ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/872'),
    # ('ì›¹ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/873'),
    # ('í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/669'),
    # ('ìžë°” ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/660'),
    # ('íŒŒì´ì¬ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/899'),
    # ('ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/1634'),
    # ('C,C++ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/900'),
    # ('DevOps / ì‹œìŠ¤í…œ ê´€ë¦¬ìž', 'https://www.wanted.co.kr/wdlist/518/674'),
    # ('ì‹œìŠ¤í…œ,ë„¤íŠ¸ì›Œí¬ ê´€ë¦¬ìž', 'https://www.wanted.co.kr/wdlist/518/665'),
    # ('ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/655'),
    # ('Node.js ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/895'),
    # ('ê°œë°œ ë§¤ë‹ˆì €', 'https://www.wanted.co.kr/wdlist/518/877'),
    # ('ìž„ë² ë””ë“œ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/658'),
    # ('QA,í…ŒìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/676'),
    # ('ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸', 'https://www.wanted.co.kr/wdlist/518/1024'),
    # ('ë¹…ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/1025'),
    # ('ì•ˆë“œë¡œì´ë“œ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/677'),
    # ('iOS ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/678'),
    # ('ê¸°ìˆ ì§€ì›', 'https://www.wanted.co.kr/wdlist/518/1026'),
    # ('í•˜ë“œì›¨ì–´ ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/672'),
    # ('í¬ë¡œìŠ¤í”Œëž«í¼ ì•± ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/10111'),
    # ('í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €', 'https://www.wanted.co.kr/wdlist/518/876'),
    # ('ë¸”ë¡ì²´ì¸ í”Œëž«í¼ ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/1027'),
    # ('DBA', 'https://www.wanted.co.kr/wdlist/518/10231'),
    # ('ì›¹ í¼ë¸”ë¦¬ì…”', 'https://www.wanted.co.kr/wdlist/518/939'),
    # ('ì˜ìƒ,ìŒì„± ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/896'),
    # ('PHP ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/893'),
    # ('.NET ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/661'),
    # ('CTO,Chief Technology Officer', 'https://www.wanted.co.kr/wdlist/518/795'),
    # ('ê·¸ëž˜í”½ìŠ¤ ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/898'),
    # ('ERPì „ë¬¸ê°€', 'https://www.wanted.co.kr/wdlist/518/10230'),
    # ('BI ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/1022'),
    # ('VR ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/10112'),
    # ('ë£¨ë¹„ì˜¨ë ˆì¼ì¦ˆ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/518/894'),
    # ('í…Œí¬ë‹ˆì»¬ ë¼ì´í„°', 'https://www.wanted.co.kr/wdlist/518/10536'),
    # ('CIO,Chief Information Officer', 'https://www.wanted.co.kr/wdlist/518/793'),
    # ('RPA ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/518/10531'),

   # ê²Œìž„ ì¹´í…Œê³ ë¦¬ ì¶”ê°€,   enumerate(list_data) ë³€ê²½
     #('ê²Œìž„ ê¸°íšìž', 'https://www.wanted.co.kr/wdlist/959/892'),
     #('ê²Œìž„ ê·¸ëž˜í”½ ë””ìžì´ë„ˆ', 'https://www.wanted.co.kr/wdlist/959/880'),
     #('ê²Œìž„ í´ë¼ì´ì–¸íŠ¸ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/959/961'),
     #('ê²Œìž„ ì•„í‹°ìŠ¤íŠ¸', 'https://www.wanted.co.kr/wdlist/959/881'),
     #('ê²Œìž„ ì„œë²„ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/959/960'),
     #('ëª¨ë°”ì¼ ê²Œìž„ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/959/962'),
     #('ì–¸ë¦¬ì–¼ ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/959/897'),
     #('ìœ ë‹ˆí‹° ê°œë°œìž', 'https://www.wanted.co.kr/wdlist/959/878'),
     #('ê²Œìž„ìš´ì˜ìž(GM)', 'https://www.wanted.co.kr/wdlist/959/958'),

    # ì œì¡°Â·ìƒì‚°
    ('í’ˆì§ˆ ê´€ë¦¬ìž', 'https://www.wanted.co.kr/wdlist/522/704'),
    ('ìƒì‚° ê´€ë¦¬ìž', 'https://www.wanted.co.kr/wdlist/522/701'),
    ('ìžìž¬ê´€ë¦¬Â·êµ¬ë§¤', 'https://www.wanted.co.kr/wdlist/522/699'),
    ('ê¸°ê³„Â·ì„¤ë¹„Â·ì„¤ê³„', 'https://www.wanted.co.kr/wdlist/522/700'),
    ('ì„¬ìœ Â·ì˜ë¥˜Â·íŒ¨ì…˜', 'https://www.wanted.co.kr/wdlist/522/10113'),
    ('ê³µì • ê´€ë¦¬ìž', 'https://www.wanted.co.kr/wdlist/522/703'),
    ('ì œì¡° ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/522/698'),
    ('ìƒì‚°ì§ ì¢…ì‚¬ìž', 'https://www.wanted.co.kr/wdlist/522/702'),
    ('ë°˜ë„ì²´Â·ë””ìŠ¤í”Œë ˆì´', 'https://www.wanted.co.kr/wdlist/522/10114'),
    ('ì•ˆì „ ê´€ë¦¬ìž', 'https://www.wanted.co.kr/wdlist/522/705'),
    ('í™”í•™ìž', 'https://www.wanted.co.kr/wdlist/522/696'),
    ('ê¸°ê³„ì œìž‘ ê¸°ìˆ ìž', 'https://www.wanted.co.kr/wdlist/522/697'),
    ('ì¡°ë¦½ ê¸°ìˆ ìž', 'https://www.wanted.co.kr/wdlist/522/695'),
    ('ì œì¡° í…ŒìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´', 'https://www.wanted.co.kr/wdlist/522/706'),

    
]

MAX_WORKERS = 10  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜

# ì—‘ì…€ ë¶ˆë²• ë¬¸ìž ì œê±°ìš©
ILLEGAL_CHARS_RE = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f]')

def clean_illegal_chars(value):
    """ì—‘ì…€ì—ì„œ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ì œì–´ ë¬¸ìž ì œê±°"""
    if isinstance(value, str):
        return ILLEGAL_CHARS_RE.sub('', value)
    return value

# ============================================
# ìƒì„¸ íŽ˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜
# ============================================
def crawl_detail_page(row_data, progress_lock, progress_info):
    """ìƒì„¸ íŽ˜ì´ì§€ í¬ë¡¤ë§ (requests + __NEXT_DATA__ JSON)"""
    
    idx = row_data['idx']
    href = row_data['link']
    
    result = {
        'idx': idx,
        'position': '',
        'content1': '',
        'content2': '',
        'content3': '-',
        'content4': '-',
        'period': '-',
        'skill': ''
    }
    
    if pd.isna(href) or not href:
        return result
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9',
        }
        
        response = requests.get(href, headers=headers, timeout=10)
        
        if response.status_code != 200:
            return result
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # __NEXT_DATA__ì—ì„œ JSON ì¶”ì¶œ
        next_data_script = soup.find('script', id='__NEXT_DATA__')
        
        if next_data_script:
            try:
                json_data = json.loads(next_data_script.string)
                props = json_data.get('props', {})
                page_props = props.get('pageProps', {})
                job_detail = page_props.get('jobDetail', {})
                
                # í¬ì§€ì…˜ ìƒì„¸ (description í•„ë“œì—ì„œ ê°€ì ¸ì˜´)
                description = job_detail.get('description', '')
                result['position'] = description.replace('\n', ' ').strip() if description else ''
                
                # ì£¼ìš”ì—…ë¬´, ìžê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­, í˜œíƒ
                detail = job_detail.get('detail', {})
                
                # ì£¼ìš”ì—…ë¬´
                intro = detail.get('intro', '')
                result['content1'] = intro.replace('\n', ' ').replace('â€¢ ', '').strip() if intro else ''
                
                # ìžê²©ìš”ê±´
                requirements = detail.get('requirements', '')
                result['content2'] = requirements.replace('\n', ' ').replace('â€¢ ', '').strip() if requirements else ''
                
                # ìš°ëŒ€ì‚¬í•­
                preferred = detail.get('preferred', '')
                result['content3'] = preferred.replace('\n', ' ').replace('â€¢ ', '').strip() if preferred else '-'
                
                # í˜œíƒ ë° ë³µì§€
                benefits = detail.get('benefits', '')
                result['content4'] = benefits.replace('\n', ' ').replace('â€¢ ', '').strip() if benefits else '-'
                
                # ë§ˆê°ì¼
                due_time = job_detail.get('dueTime', '')
                result['period'] = due_time if due_time else '-'
                
                # ê¸°ìˆ ìŠ¤íƒ
                skill_tags = job_detail.get('skillTags', [])
                if skill_tags:
                    result['skill'] = '::'.join([tag.get('name', '') for tag in skill_tags if tag.get('name')])
                
            except json.JSONDecodeError:
                pass
        
        # JSONì´ ì—†ìœ¼ë©´ HTML íŒŒì‹± í´ë°±
        # í¬ì§€ì…˜ ìƒì„¸ í´ë°±
        if not result['position']:
            try:
                # article.JobDescription_JobDescription__s2Keo ë‚´ë¶€ì˜ ì²« ë²ˆì§¸ span.wds-h4ga6o ë‚´ìš©
                article = soup.find('article', class_=lambda x: x and 'JobDescription_JobDescription' in x)
                if article:
                    # ì£¼ìš”ì—…ë¬´/ìžê²©ìš”ê±´ ë“± h3 íƒœê·¸ê°€ ìžˆëŠ” div ì´ì „ì˜ span ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                    wrapper = article.find('div', class_=lambda x: x and 'paragraph__wrapper' in x)
                    if wrapper:
                        first_span = wrapper.find('span', class_='wds-h4ga6o', recursive=False)
                        if not first_span:
                            first_span = wrapper.find('span', class_='wds-h4ga6o')
                        if first_span:
                            # h3 íƒœê·¸ê°€ ì—†ëŠ” ì²« ë²ˆì§¸ span ë‚´ìš©ë§Œ ê°€ì ¸ì˜¤ê¸°
                            inner_span = first_span.find('span')
                            if inner_span and not inner_span.find('h3'):
                                result['position'] = inner_span.get_text(' ', strip=True).replace('â€¢ ', '')
            except:
                pass
        
        if not result['content1']:
            try:
                content1_div = soup.find('h3', string=lambda x: x and 'ì£¼ìš”ì—…ë¬´' in x)
                if content1_div:
                    parent = content1_div.find_parent('div')
                    if parent:
                        result['content1'] = parent.get_text(' ', strip=True).replace('ì£¼ìš”ì—…ë¬´', '').replace('â€¢ ', '').strip()
            except:
                pass
            
            try:
                content2_div = soup.find('h3', string=lambda x: x and 'ìžê²©ìš”ê±´' in x)
                if content2_div:
                    parent = content2_div.find_parent('div')
                    if parent:
                        result['content2'] = parent.get_text(' ', strip=True).replace('ìžê²©ìš”ê±´', '').replace('â€¢ ', '').strip()
            except:
                pass
    
    except Exception as e:
        pass
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    with progress_lock:
        progress_info['completed'] += 1
        total = progress_info['total']
        completed = progress_info['completed']
        status = "âœ“" if result['content1'] or result['content2'] else "-"
        print(f"\r  [{completed}/{total}] ìƒì„¸ í¬ë¡¤ë§ ì¤‘... ({completed/total*100:.0f}%) {status}", end='', flush=True)
    
    return result


def crawl_job_list(job_category, url, driver):
    """íŠ¹ì • ì§ë¬´ ì¹´í…Œê³ ë¦¬ì˜ ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ (Selenium)"""
    
    driver.get(url)
    time.sleep(3)
    
    # XPath ì„ íƒìž
    element_xpath = "//div[@data-cy='job-card']/a"
    alternative_xpaths = [
        "//div[@data-cy='job-card']/a",
        "//a[contains(@href, '/wd/')]",
        "//div[contains(@class, 'JobCard_JobCard')]/a",
    ]
    
    wait = WebDriverWait(driver, 20)
    
    # ìš”ì†Œ ì°¾ê¸°
    element_found = False
    for xpath in alternative_xpaths:
        try:
            wait.until(EC.presence_of_element_located((By.XPATH, xpath)))
            element_xpath = xpath
            element_found = True
            break
        except Exception:
            continue
    
    if not element_found:
        element_xpath = alternative_xpaths[0]
    
    # ìŠ¤í¬ë¡¤ ë‹¤ìš´
    SCROLL_PAUSE_TIME = 1.5
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
    except InvalidSessionIdException:
        driver.quit()
        return []
    
    same_count = 0
    
    while True:
        try:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(SCROLL_PAUSE_TIME)
            new_height = driver.execute_script("return document.body.scrollHeight")
        except (InvalidSessionIdException, WebDriverException):
            break
        
        if new_height == last_height:
            same_count += 1
        else:
            same_count = 0
        
        if same_count >= 2:
            break
        
        last_height = new_height
    
    # ìš”ì†Œ ìˆ˜ì§‘
    elements = []
    try:
        elements = driver.find_elements(By.XPATH, element_xpath)
    except Exception:
        elements = []
    
    # ë¦¬ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘
    list_data = []
    for idx, e in enumerate(elements):
        try:
            max_retries = 3
            retry_count = 0
            href = None
            job_category_id = ''
            company_id = ''
            company_name = ''
            position_name = ''
            position_id = ''
            
            while retry_count < max_retries:
                try:
                    href = e.get_attribute('href')
                    if href and href.startswith('/'):
                        href = f"https://www.wanted.co.kr{href}"
                    
                    parent_div = e.find_element(By.XPATH, "./..")
                    try:
                        button = parent_div.find_element(By.XPATH, ".//button[@data-attribute-id='position__bookmark__click']")
                        job_category_id = button.get_attribute('data-job-category-id') or ''
                        company_id = button.get_attribute('data-company-id') or ''
                        company_name = button.get_attribute('data-company-name') or ''
                        position_name = button.get_attribute('data-position-name') or ''
                        position_id = button.get_attribute('data-position-id') or ''
                    except:
                        pass
                    break
                except StaleElementReferenceException:
                    retry_count += 1
                    if retry_count < max_retries:
                        elements = driver.find_elements(By.XPATH, element_xpath)
                        if idx < len(elements):
                            e = elements[idx]
                        time.sleep(0.5)
                    else:
                        raise
            
            if href is None:
                continue
            
            list_data.append({
                'job_category_id': job_category_id,
                'job_category': job_category,
                'company_id': company_id,
                'company_name': company_name,
                'position_name': position_name,
                'position_id': position_id,
                'link': href or '',
            })
        
        except Exception:
            continue
    
    return list_data


def crawl_detail_pages(list_data):
    """ìƒì„¸ íŽ˜ì´ì§€ ë³‘ë ¬ í¬ë¡¤ë§"""
    
    if not list_data:
        return []
    
    # ì§„í–‰ ìƒí™© ì¶”ì 
    progress_lock = threading.Lock()
    progress_info = {'completed': 0, 'total': len(list_data)}
    
    # row_data ì¤€ë¹„
    row_data_list = []
    for idx, row in enumerate(list_data):
        row_data_list.append({
            'idx': idx,
            'link': row['link'],
            'company_name': row.get('company_name', ''),
        })
    
    detail_results = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(crawl_detail_page, rd, progress_lock, progress_info) for rd in row_data_list]
        
        for future in as_completed(futures):
            detail_results.append(future.result())
    
    # ê²°ê³¼ ì •ë ¬ (ì›ëž˜ ìˆœì„œëŒ€ë¡œ)
    detail_results.sort(key=lambda x: x['idx'])
    
    print()  # ì¤„ë°”ê¿ˆ
    
    return detail_results


def merge_list_and_detail(list_data, detail_results):
    """ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ì™€ ìƒì„¸ ë°ì´í„° ë³‘í•©"""
    
    merged_data = []
    
    for i, (list_item, detail_item) in enumerate(zip(list_data, detail_results)):
        merged = {**list_item}
        merged['position'] = detail_item.get('position', '')
        merged['content1'] = detail_item.get('content1', '')
        merged['content2'] = detail_item.get('content2', '')
        merged['content3'] = detail_item.get('content3', '-')
        merged['content4'] = detail_item.get('content4', '-')
        merged['period'] = detail_item.get('period', '-')
        merged['skill'] = detail_item.get('skill', '')
        merged_data.append(merged)
    
    return merged_data


# ============================================
# ë©”ì¸ ì‹¤í–‰
# ============================================
if __name__ == "__main__":
    print("=" * 80)
    print("ðŸš€ ì›í‹°ë“œ ì±„ìš©ê³µê³  í¬ë¡¤ë§ (ì „ì²´ ì§ë¬´ ì¹´í…Œê³ ë¦¬)")
    print(f"   ì´ {len(JOB_CATEGORIES)}ê°œ ì§ë¬´ ì¹´í…Œê³ ë¦¬")
    print("=" * 80)
    
    now_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    start_total_time = time.time()
    
    # Chrome ë“œë¼ì´ë²„ í•œ ë²ˆë§Œ ì´ˆê¸°í™” (ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ ìž¬ì‚¬ìš©)
    print("\nðŸŒ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--start-maximized')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    # chrome_options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì›í•˜ë©´ ì£¼ì„ í•´ì œ
    
    driver = webdriver.Chrome(options=chrome_options)
    print("âœ“ Chrome ë“œë¼ì´ë²„ ì¤€ë¹„ ì™„ë£Œ")
    
    all_data = []  # ëª¨ë“  ì§ë¬´ì˜ ë°ì´í„°ë¥¼ ëˆ„ì 
    
    try:
        for category_idx, (job_category, url) in enumerate(JOB_CATEGORIES, 1):
            print(f"\n{'='*80}")
            print(f"[{category_idx}/{len(JOB_CATEGORIES)}] ðŸ“‹ {job_category}")
            print(f"URL: {url}")
            print("="*80)
            
            # 1ë‹¨ê³„: ë¦¬ìŠ¤íŠ¸ íŽ˜ì´ì§€ í¬ë¡¤ë§
            print("\n[1ë‹¨ê³„] ë¦¬ìŠ¤íŠ¸ íŽ˜ì´ì§€ ìŠ¤í¬ë¡¤ ë° ìˆ˜ì§‘ ì¤‘...")
            start_time = time.time()
            list_data = crawl_job_list(job_category, url, driver)
            list_time = time.time() - start_time
            print(f"âœ“ {len(list_data)}ê°œ ê³µê³  ë°œê²¬ ({list_time:.1f}ì´ˆ)")
            
            if not list_data:
                print("âš  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                continue
            
            # 2ë‹¨ê³„: ìƒì„¸ íŽ˜ì´ì§€ ë³‘ë ¬ í¬ë¡¤ë§
            print(f"\n[2ë‹¨ê³„] ìƒì„¸ íŽ˜ì´ì§€ ë³‘ë ¬ í¬ë¡¤ë§ ({MAX_WORKERS}ê°œ ë™ì‹œ ì²˜ë¦¬)")
            start_time = time.time()
            detail_results = crawl_detail_pages(list_data)
            detail_time = time.time() - start_time
            
            content_count = sum(1 for r in detail_results if r.get('content1') or r.get('content2'))
            print(f"âœ“ ìƒì„¸ í¬ë¡¤ë§ ì™„ë£Œ: {content_count}/{len(detail_results)}ê°œ ({detail_time:.1f}ì´ˆ)")
            
            # 3ë‹¨ê³„: ë°ì´í„° ë³‘í•©
            merged_data = merge_list_and_detail(list_data, detail_results)
            all_data.extend(merged_data)
            
            print(f"âœ“ ëˆ„ì  ë°ì´í„°: {len(all_data)}ê°œ")
    
    finally:
        # Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ
        print("\nðŸ”š Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘...")
        driver.quit()
        print("âœ“ Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")
    
    # ============================================
    # ìµœì¢… ì €ìž¥
    # ============================================
    print("\n" + "=" * 80)
    print("ðŸ“Š ìµœì¢… ê²°ê³¼ ì €ìž¥ ì¤‘...")
    print("=" * 80)
    
    df_final = pd.DataFrame(all_data)
    
    # ë¶ˆë²• ë¬¸ìž ì œê±°
    print("  â†’ ë¶ˆë²• ë¬¸ìž ì •ë¦¬ ì¤‘...")
    for col in df_final.columns:
        if df_final[col].dtype == 'object':
            df_final[col] = df_final[col].apply(clean_illegal_chars)
    
    # ìµœì¢… ì €ìž¥
    final_save_path = f"C://Users//MULTICAMPUS//Desktop//curosr-playground//wanted//wanted_all_jobs_{now_str}.xlsx"
    df_final.to_excel(final_save_path, index=False, engine='openpyxl')
    
    total_elapsed_time = time.time() - start_total_time
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    print("\n" + "=" * 80)
    print("ðŸ“Š í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 80)
    print(f"ì´ ì§ë¬´ ì¹´í…Œê³ ë¦¬: {len(JOB_CATEGORIES)}ê°œ")
    print(f"ì´ ê³µê³  ìˆ˜: {len(df_final)}ê°œ")
    
    # ì§ë¬´ë³„ í†µê³„
    print("\nðŸ“‹ ì§ë¬´ë³„ ê³µê³  ìˆ˜:")
    category_counts = df_final['job_category'].value_counts()
    for cat, cnt in category_counts.items():
        print(f"  - {cat}: {cnt}ê°œ")
    
    print(f"\nì´ ì†Œìš” ì‹œê°„: {total_elapsed_time/60:.1f}ë¶„ ({total_elapsed_time:.0f}ì´ˆ)")
    print(f"\nâœ“ ìµœì¢… íŒŒì¼: {final_save_path}")
    print("\nðŸŽ‰ ì „ì²´ ìž‘ì—… ì™„ë£Œ!")
